{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 후 동적 범위 양자화\n",
    "\n",
    "* https://www.tensorflow.org/lite/performance/post_training_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU[0] is ready\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU[0] is ready')\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)\n",
    "else:\n",
    "    print('Please check GPU available')\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반 학습으로 MNIST 모델 생성 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 855us/step - loss: 0.2697 - accuracy: 0.9268 - val_loss: 0.1126 - val_accuracy: 0.9684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfa4e55c88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=1,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite 모델로 변환하기  (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/logs/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hoondori/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/hoondori/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp7yavggdg/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 양자화된 모델로 저장하기 => 1/4 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7xb2dhtv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7xb2dhtv/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23840"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dtype: <class 'numpy.float32'>, output dtype: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# 입출력은 여전히 FP32\n",
    "in_dtype = interpreter_quant.get_input_details()[0]['dtype']\n",
    "out_dtype = interpreter_quant.get_output_details()[0]['dtype']\n",
    "print(f'input dtype: {in_dtype}, output dtype: {out_dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite 모델 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image shape: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "print(f'test image shape: {test_images[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index = interpreter_quant.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter_quant.get_output_details()[0][\"index\"]\n",
    "interpreter_quant.set_tensor(input_index, test_image)\n",
    "interpreter_quant.invoke()\n",
    "predictions = interpreter_quant.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.imshow(test_images[0])\n",
    "template = \"True:{true}, predicted:{predict}\"\n",
    "_ = plt.title(template.format(true= str(test_labels[0]),\n",
    "                              predict=str(np.argmax(predictions[0]))))\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP32 모델 INT8 양자화 모델 평가 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32 acc: 0.9684, INT8 acc: 0.9684\n"
     ]
    }
   ],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for test_image in test_images:\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    accurate_count = 0\n",
    "    for index in range(len(prediction_digits)):\n",
    "        if prediction_digits[index] == test_labels[index]:\n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "eval_fp32 = evaluate_model(interpreter)\n",
    "eval_quant = evaluate_model(interpreter_quant)\n",
    "print(f'fp32 acc: {eval_fp32}, INT8 acc: {eval_quant}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
