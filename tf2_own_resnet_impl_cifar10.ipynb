{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY\n",
    "%load_ext tensorboard\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 로 Resnet 자체 구현 및 성능 측정 \n",
    "\n",
    "* 참고 자료 \n",
    "  * https://towardsdatascience.com/understand-and-implement-resnet-50-with-tensorflow-2-0-1190b9b52691\n",
    "  * https://github.com/calmisential/TensorFlow2.0_ResNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (baseline) Small CNN CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3686912   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,692,938\n",
      "Trainable params: 3,692,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 1.7152 - accuracy: 0.3951 - val_loss: 1.4374 - val_accuracy: 0.4989\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 1.3702 - accuracy: 0.5170 - val_loss: 1.2717 - val_accuracy: 0.5606\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 1.2378 - accuracy: 0.5671 - val_loss: 1.3586 - val_accuracy: 0.5429\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 1.1431 - accuracy: 0.6018 - val_loss: 1.1232 - val_accuracy: 0.6150\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 1.0666 - accuracy: 0.6270 - val_loss: 1.1053 - val_accuracy: 0.6212\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 1.0117 - accuracy: 0.6475 - val_loss: 1.0750 - val_accuracy: 0.6240\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.9537 - accuracy: 0.6680 - val_loss: 1.0242 - val_accuracy: 0.6509\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.9060 - accuracy: 0.6845 - val_loss: 1.0972 - val_accuracy: 0.6295\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.8597 - accuracy: 0.7023 - val_loss: 0.9915 - val_accuracy: 0.6673\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.8164 - accuracy: 0.7175 - val_loss: 1.0015 - val_accuracy: 0.6679\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.7736 - accuracy: 0.7325 - val_loss: 1.1226 - val_accuracy: 0.6371\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.7517 - accuracy: 0.7392 - val_loss: 1.0117 - val_accuracy: 0.6659\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.7146 - accuracy: 0.7540 - val_loss: 1.0108 - val_accuracy: 0.6681\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.6833 - accuracy: 0.7649 - val_loss: 1.0533 - val_accuracy: 0.6698\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.6571 - accuracy: 0.7736 - val_loss: 0.9839 - val_accuracy: 0.6854\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.7851 - val_loss: 1.0316 - val_accuracy: 0.6764\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.6054 - accuracy: 0.7931 - val_loss: 1.1133 - val_accuracy: 0.6553\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5876 - accuracy: 0.7984 - val_loss: 1.0149 - val_accuracy: 0.6796\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5633 - accuracy: 0.8086 - val_loss: 1.0490 - val_accuracy: 0.6860\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5507 - accuracy: 0.8124 - val_loss: 1.0597 - val_accuracy: 0.6748\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 1.0526 - accuracy: 0.6717\n",
      "\n",
      "Test score: 1.0526033639907837\n",
      "Test accuracy: 0.6717000007629395\n"
     ]
    }
   ],
   "source": [
    "def cnn_cifar10(log_dir):\n",
    "\n",
    "    #define the convnet \n",
    "    def build(input_shape, classes):\n",
    "        model = models.Sequential() \n",
    "        model.add(layers.Convolution2D(32, (3, 3), activation='relu',\n",
    "                                input_shape=input_shape))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Dropout(0.25)) \n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(512, activation='relu'))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "    # CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "    IMG_CHANNELS = 3\n",
    "    IMG_ROWS = 32\n",
    "    IMG_COLS = 32\n",
    "\n",
    "    #constant\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 20\n",
    "    CLASSES = 10\n",
    "    VERBOSE = 1\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    OPTIM = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "    # data: shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "    # normalize\n",
    "    X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "    # convert to categorical\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, CLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, CLASSES)\n",
    "\n",
    "    model=build((IMG_ROWS, IMG_COLS, IMG_CHANNELS), CLASSES)\n",
    "    model.summary()\n",
    "\n",
    "    # use TensorBoard, princess Aurora!\n",
    "    callbacks = [\n",
    "      # Write TensorBoard logs to `./logs` directory\n",
    "      tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    ]\n",
    "\n",
    "    # train\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS, validation_split=VALIDATION_SPLIT, \n",
    "        verbose=VERBOSE, callbacks=callbacks) \n",
    "    score = model.evaluate(X_test, y_test,\n",
    "                         batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "    print(\"\\nTest score:\", score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "!rm -rf /tmp/logs/cnn_cifar10     \n",
    "cnn_cifar10('/tmp/logs/cnn_cifar10')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet-50 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     10,
     38,
     73
    ]
   },
   "outputs": [],
   "source": [
    "#### Necessary Imports for Neural Net \n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, ZeroPadding2D,\\\n",
    "     Flatten, BatchNormalization, AveragePooling2D, Dense, Activation, Add \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def res_identity(x, filters):\n",
    "\n",
    "    #  x => 1x1xf1 => 3x3xf1 => 1x1xf2 => x + residual \n",
    "    \n",
    "    x_skip = x # for residual learning\n",
    "    f1, f2 = filters \n",
    "    \n",
    "    # first block (1x1xf1 s1)\n",
    "    x = Conv2D(f1, kernel_size=(1,1), strides=(1,1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    \n",
    "    # second block (3x3xf1 s1)\n",
    "    x = Conv2D(f1, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    \n",
    "    # third block (1x1xf2 s1)\n",
    "    x = Conv2D(f2, kernel_size=(1,1), strides=(1,1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    # add the input\n",
    "    x = Add()([x, x_skip])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def res_conv(x, s, filters):\n",
    "    # 첫번째 block에서 s>1이면 x 와 residual 경로상의 크기가 달라지므로 이를 신경써서 맞추어 주어야 한다.\n",
    "    # 해법은(ex. s=2) residual 로 오는쪽에서 1x1 s=2로 보폭을 늘리고 로 w/h를 반만큼 줄이고, output channel을 2배로 늘려야 한다.\n",
    "    \n",
    "    \n",
    "    #  x => 1x1xf1 s2 => 3x3xf1 s1=> 1x1xf2 s1=> x + residual( by 1x1xf2 s2) \n",
    "    \n",
    "    x_skip = x\n",
    "    f1, f2 = filters\n",
    "    \n",
    "    # first block (1x1xf1 s>1)\n",
    "    x = Conv2D(f1, kernel_size=(1,1), strides=(s,s), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    \n",
    "    # second block (3x3xf1 s1)\n",
    "    x = Conv2D(f1, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    \n",
    "    # third block (1x1xf2 s1)\n",
    "    x = Conv2D(f2, kernel_size=(1,1), strides=(1,1), padding='valid', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    \n",
    "    # skip connection \n",
    "    x_skip = Conv2D(f2, kernel_size=(1,1), strides=(s,s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)\n",
    "    x_skip = BatchNormalization()(x_skip)\n",
    "\n",
    "    # add\n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(activations.relu)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def resnet50(input_shape, n_class):\n",
    "\n",
    "    input_im = Input(shape=input_shape) # cifar 10 images size\n",
    "    x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
    "\n",
    "    # 1st stage\n",
    "    # here we perform maxpooling, see the figure above\n",
    "\n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    #2nd stage \n",
    "    # frm here on only conv block and identity block, no pooling\n",
    "\n",
    "    x = res_conv(x, s=1, filters=(64, 256))\n",
    "    x = res_identity(x, filters=(64, 256))\n",
    "    x = res_identity(x, filters=(64, 256))\n",
    "\n",
    "    # 3rd stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "    x = res_identity(x, filters=(128, 512))\n",
    "\n",
    "    # 4th stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "    x = res_identity(x, filters=(256, 1024))\n",
    "\n",
    "    # 5th stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(512, 2048))\n",
    "    x = res_identity(x, filters=(512, 2048))\n",
    "    x = res_identity(x, filters=(512, 2048))\n",
    "\n",
    "    # ends with average pooling and dense connection\n",
    "\n",
    "    x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(n_class, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
    "\n",
    "    # define the model \n",
    "\n",
    "    model = Model(inputs=input_im, outputs=x, name='Resnet50')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/313 [..............................] - ETA: 1:06 - loss: 24.5582 - accuracy: 0.0859WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.189092). Check your callbacks.\n",
      "313/313 [==============================] - 18s 59ms/step - loss: 10.5917 - accuracy: 0.2984 - val_loss: 6.9804 - val_accuracy: 0.1227\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 16s 53ms/step - loss: 5.2936 - accuracy: 0.3489 - val_loss: 22.7027 - val_accuracy: 0.1063\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 4.5113 - accuracy: 0.3478 - val_loss: 213.0576 - val_accuracy: 0.1683\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 16s 53ms/step - loss: 3.6823 - accuracy: 0.4129 - val_loss: 3.7933 - val_accuracy: 0.2173\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 16s 53ms/step - loss: 3.3536 - accuracy: 0.4340 - val_loss: 3.6055 - val_accuracy: 0.3927\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 3.1226 - accuracy: 0.4461 - val_loss: 3.6890 - val_accuracy: 0.2319\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 2.7756 - accuracy: 0.4688 - val_loss: 23.5130 - val_accuracy: 0.2604\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 2.5727 - accuracy: 0.4978 - val_loss: 369.0206 - val_accuracy: 0.2024\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 2.4002 - accuracy: 0.5270 - val_loss: 6.5455 - val_accuracy: 0.1843\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 2.4387 - accuracy: 0.5305 - val_loss: 2.1692 - val_accuracy: 0.4701\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 2.2557 - accuracy: 0.5429 - val_loss: 2.2836 - val_accuracy: 0.4217\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.9470 - accuracy: 0.5921 - val_loss: 3.4706 - val_accuracy: 0.2161\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.7974 - accuracy: 0.6122 - val_loss: 18.6913 - val_accuracy: 0.3325\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.6496 - accuracy: 0.6374 - val_loss: 2.3197 - val_accuracy: 0.4848\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.6642 - accuracy: 0.6436 - val_loss: 1.5795 - val_accuracy: 0.6132\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.5281 - accuracy: 0.6617 - val_loss: 5.3108 - val_accuracy: 0.5033\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.5240 - accuracy: 0.6766 - val_loss: 2.1991 - val_accuracy: 0.5138\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.4329 - accuracy: 0.6882 - val_loss: 4.2900 - val_accuracy: 0.5025\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.4426 - accuracy: 0.7016 - val_loss: 28.6605 - val_accuracy: 0.5366\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 1.3662 - accuracy: 0.7150 - val_loss: 164.0336 - val_accuracy: 0.5826\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 183.4448 - accuracy: 0.5764\n",
      "\n",
      "Test score: 183.44482421875\n",
      "Test accuracy: 0.5763999819755554\n"
     ]
    }
   ],
   "source": [
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "log_dir = '/tmp/logs/resnet50'\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "# normalize\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "# convert to categorical\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, CLASSES)\n",
    "\n",
    "model=resnet50((IMG_ROWS, IMG_COLS, IMG_CHANNELS), CLASSES)\n",
    "#model.summary()\n",
    "\n",
    "# use TensorBoard, princess Aurora!\n",
    "callbacks = [\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "]\n",
    "\n",
    "# train\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS, validation_split=VALIDATION_SPLIT, \n",
    "    verbose=VERBOSE, callbacks=callbacks) \n",
    "score = model.evaluate(X_test, y_test,\n",
    "                     batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
