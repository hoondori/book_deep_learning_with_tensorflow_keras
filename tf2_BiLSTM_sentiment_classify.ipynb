{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 기반 Bi-LSTM 모델로 Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY\n",
    "%load_ext tensorboard\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \n",
      "[0] Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \n",
      "[0] Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  \n",
      "[0] Very little music or anything to speak of.  \n",
      "[1] The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  \n"
     ]
    }
   ],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace(\"%20\", \" \")\n",
    "    p = tf.keras.utils.get_file(local_file, url, extract=True)   # ~/.keras/datasets\n",
    "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
    "            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences\n",
    "\n",
    "# download and read data into data structures\n",
    "labeled_sentences = download_and_read(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\")\n",
    "sentences = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s, l) in labeled_sentences]\n",
    "for i in range(5):\n",
    "    print(f'[{labels[i]}] {sentences[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5271\n",
      "tf.Tensor(\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    4   17   17   17  230  746\n",
      " 2267   24   54    4 2268 2269  878  324], shape=(64,), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# tokenizer로 integer list로 변환 후 tf.dataset 준비\n",
    "\n",
    "MAX_SEQLEN = 64\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(\"vocabulary size: {:d}\".format(vocab_size))\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for (k, v) in word2idx.items()}\n",
    "idx2word[0] = 'PAD'\n",
    "\n",
    "# create dataset\n",
    "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
    "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(sentences_as_ints, maxlen=MAX_SEQLEN)\n",
    "labels = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_ints, labels))\n",
    "for sentence, label in dataset.take(1):\n",
    "    print(sentence)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 64), (None,)), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/valid/test 분리\n",
    "\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences) // 3\n",
    "val_size = (len(sentences) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      multiple                  337408    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection multiple                  66048     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 411,777\n",
      "Trainable params: 411,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_sz, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, emb_sz)\n",
    "        \n",
    "        self.bilstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(emb_sz)\n",
    "        )\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "EMBED_SZ = 64\n",
    "model = SentimentAnalysisModel(vocab_size+1, EMBED_SZ) # vocab_size + 1 to account for PAD character\n",
    "model.build(input_shape=(BATCH_SIZE, MAX_SEQLEN))\n",
    "model.summary()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/57 [===========================>..] - ETA: 0s - loss: 0.6853 - accuracy: 0.5557\n",
      "Epoch 00001: val_loss improved from inf to 0.62423, saving model to /tmp/logs/BiLSTM/cp-0001.ckpt\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.6841 - accuracy: 0.5611 - val_loss: 0.6242 - val_accuracy: 0.6500\n",
      "Epoch 2/10\n",
      "50/57 [=========================>....] - ETA: 0s - loss: 0.4849 - accuracy: 0.7781\n",
      "Epoch 00002: val_loss improved from 0.62423 to 0.27867, saving model to /tmp/logs/BiLSTM/cp-0002.ckpt\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.4684 - accuracy: 0.7894 - val_loss: 0.2787 - val_accuracy: 0.8950\n",
      "Epoch 3/10\n",
      "51/57 [=========================>....] - ETA: 0s - loss: 0.2803 - accuracy: 0.8842\n",
      "Epoch 00003: val_loss improved from 0.27867 to 0.18607, saving model to /tmp/logs/BiLSTM/cp-0003.ckpt\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.2728 - accuracy: 0.8878 - val_loss: 0.1861 - val_accuracy: 0.9350\n",
      "Epoch 4/10\n",
      "50/57 [=========================>....] - ETA: 0s - loss: 0.1616 - accuracy: 0.9356\n",
      "Epoch 00004: val_loss improved from 0.18607 to 0.11631, saving model to /tmp/logs/BiLSTM/cp-0004.ckpt\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.1572 - accuracy: 0.9372 - val_loss: 0.1163 - val_accuracy: 0.9700\n",
      "Epoch 5/10\n",
      "51/57 [=========================>....] - ETA: 0s - loss: 0.1351 - accuracy: 0.9614\n",
      "Epoch 00005: val_loss improved from 0.11631 to 0.08209, saving model to /tmp/logs/BiLSTM/cp-0005.ckpt\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.1334 - accuracy: 0.9617 - val_loss: 0.0821 - val_accuracy: 0.9800\n",
      "Epoch 6/10\n",
      "50/57 [=========================>....] - ETA: 0s - loss: 0.1038 - accuracy: 0.9725\n",
      "Epoch 00006: val_loss improved from 0.08209 to 0.07620, saving model to /tmp/logs/BiLSTM/cp-0006.ckpt\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.1003 - accuracy: 0.9728 - val_loss: 0.0762 - val_accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "50/57 [=========================>....] - ETA: 0s - loss: 0.0590 - accuracy: 0.9850\n",
      "Epoch 00007: val_loss improved from 0.07620 to 0.02619, saving model to /tmp/logs/BiLSTM/cp-0007.ckpt\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9844 - val_loss: 0.0262 - val_accuracy: 0.9950\n",
      "Epoch 8/10\n",
      "56/57 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9821\n",
      "Epoch 00008: val_loss did not improve from 0.02619\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.0556 - accuracy: 0.9822 - val_loss: 0.0492 - val_accuracy: 0.9950\n",
      "Epoch 9/10\n",
      "49/57 [========================>.....] - ETA: 0s - loss: 0.0483 - accuracy: 0.9860\n",
      "Epoch 00009: val_loss did not improve from 0.02619\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.0462 - accuracy: 0.9867 - val_loss: 0.0327 - val_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "49/57 [========================>.....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9943\n",
      "Epoch 00010: val_loss improved from 0.02619 to 0.00785, saving model to /tmp/logs/BiLSTM/cp-0010.ckpt\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.0246 - accuracy: 0.9950 - val_loss: 0.0079 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# compile\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "data_dir = '/tmp/logs/BiLSTM'\n",
    "\n",
    "# train\n",
    "\n",
    "# 개선된 epoch 마다 모델 저장\n",
    "checkpoint_path = data_dir + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "    save_weights_only=True,verbose=1, save_freq='epoch', save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=data_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs=num_epochs, \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/logs/BiLSTM/cp-0010.ckpt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = tf.train.latest_checkpoint(os.path.dirname(checkpoint_path))\n",
    "best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 집합에 대한 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9930\n",
      "test loss : 0.030, test accuracy 0.993\n"
     ]
    }
   ],
   "source": [
    "best_model = SentimentAnalysisModel(vocab_size+1, EMBED_SZ) # vocab_size + 1 to account for PAD character\n",
    "best_model.build(input_shape=(BATCH_SIZE, MAX_SEQLEN))\n",
    "best_model.load_weights(best_model_path)\n",
    "best_model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print('test loss : {:.3f}, test accuracy {:.3f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBL\tPRD:\tSENT\n",
      "0\t0\tthis is the first phone i've had that has been so cheaply made\n",
      "0\t0\tthe phone takes forever to charge like 2 to 5 hours literally\n",
      "0\t0\twas not happy\n",
      "1\t1\tgreat brunch spot\n",
      "1\t1\tthere was a warm feeling with the service and i felt like their guest for a special treat\n",
      "0\t0\toverall this movie was cheap trash\n",
      "1\t1\tthe food is good\n",
      "1\t1\tthe rest of the cast also play well\n",
      "0\t0\tafter i pulled up my car i waited for another 15 minutes before being acknowledged\n",
      "1\t1\tgo to place for gyros\n",
      "0\t0\twill never ever go back\n",
      "1\t1\tmy boyfriend and i came here for the first time on a recent trip to vegas and could not have been more pleased with the quality of food and service\n",
      "1\t1\tthis place is amazing\n",
      "1\t1\tthe first time i ever came here i had an amazing experience i still tell people how awesome the duck was\n",
      "0\t0\ttoday the graphics are crap\n",
      "1\t1\tlove this product\n",
      "0\t0\tpainful on the ear\n",
      "0\t0\tso i am here to warn you do not rent this movie it is the dumbest thing you have never seen\n",
      "0\t0\ti was not impressed by this product\n",
      "1\t1\tdefinitely worth checking out\n",
      "0\t0\tat least 40min passed in between us ordering and the food arriving and it wasn't that busy\n",
      "0\t0\tthe result is a film that just don't look right\n",
      "1\t1\tkris kristoffersen is good in this movie and really makes a difference\n",
      "1\t1\tsetup went very smoothly\n",
      "0\t0\tfood quality has been horrible\n",
      "1\t1\tall things considered a job very well done\n",
      "1\t1\tgreat product for the price\n",
      "1\t1\tseriously flavorful delights folks\n",
      "0\t0\tnot only did it only confirm that the film would be unfunny and generic but it also managed to give away the entire movie and i'm not exaggerating every moment every plot point every joke is told in the trailer\n",
      "0\t0\tthe replacement died in a few weeks\n",
      "1\t1\ti love the camera it's really pretty good quality\n",
      "1\t1\tso flavorful and has just the perfect amount of heat\n",
      "accuracy score: 0.996\n",
      "confusion matrix\n",
      "[[513   3]\n",
      " [  1 483]]\n"
     ]
    }
   ],
   "source": [
    "total_labels, predictions = [], []\n",
    "is_first_batch = True\n",
    "for test_batch in test_dataset:\n",
    "    features_b, labels_b = test_batch\n",
    "    predicts_b = best_model.predict(features_b)\n",
    "    predictions.extend( [1 if pred_pp > 0.5 else 0 for pred_pp in predicts_b] )\n",
    "    total_labels.extend([l for l in labels_b.numpy()])\n",
    "    if is_first_batch:\n",
    "        print('LBL\\tPRD:\\tSENT')\n",
    "        for rid in range(features_b.shape[0]):\n",
    "            words = [idx2word[idx] for idx in features_b[rid].numpy()]\n",
    "            words = [w for w in words if w != \"PAD\"]\n",
    "            sentence = ' '.join(words)\n",
    "            print('{:d}\\t{:d}\\t{:s}'.format(total_labels[rid], predictions[rid], sentence))\n",
    "        is_first_batch = False\n",
    "\n",
    "print(\"accuracy score: {:.3f}\".format(accuracy_score(total_labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(total_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
