{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF 2.0 의 기능을 테스트 \n",
    "\n",
    "* autograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow.keras import datasets, layers, models, preprocessing\n",
    "from tensorflow.python.client import device_lib\n",
    "import timeit\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograph \n",
    "\n",
    "* tf.function decorator를 씌우면 자동 변환\n",
    "* 최상위만 씌우면 나머지는 알아서..\n",
    "* to_code로 확인\n",
    "* decorated 안한 것 대비 그래프 최적화가 되서 수 배 차이\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03972745400005806 0.01601803599987761\n",
      "def tf__tf_fn(input, state):\n",
      "    do_return = False\n",
      "    retval_ = ag__.UndefinedReturnValue()\n",
      "    with ag__.FunctionScope('tf_fn', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = fscope.mark_return_value(ag__.converted_call(cell, (input, state), None, fscope))\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "    (do_return,)\n",
      "    return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def exercise_autograph():\n",
    "\n",
    "    cell = tf.keras.layers.LSTMCell(100)\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_fn(input, state):\n",
    "        return cell(input,state)\n",
    "\n",
    "    input = tf.zeros([100,100])\n",
    "    state = [tf.zeros([100,100])] * 2\n",
    "    \n",
    "    # 워밍업\n",
    "    cell(input, state)\n",
    "    tf_fn(input, state)\n",
    "\n",
    "    graph_time = timeit.timeit(lambda: cell(input, state), number=100)\n",
    "    autograph_time = timeit.timeit(lambda: tf_fn(input, state), number=100)\n",
    "    \n",
    "    print(graph_time, autograph_time)\n",
    "\n",
    "    print(tf.autograph.to_code(tf_fn.python_function, experimental_optional_features=None))\n",
    "    \n",
    "exercise_autograph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras의 함수적 API 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    \n",
    "    text_input_a = tf.keras.Input(shape=(None, ), dtype='int32')\n",
    "    text_input_b = tf.keras.Input(shape=(None, ), dtype='int32')\n",
    "    \n",
    "    shared_emb = tf.keras.layers.Embedding(1000,128)\n",
    "    \n",
    "    enc_input_a = shared_emb(text_input_a)\n",
    "    enc_input_b = shared_emb(text_input_b)\n",
    "    \n",
    "    pred_a = tf.keras.layers.Dense(1, activation='sigmoid', \n",
    "        name='pred_a')(enc_input_a)\n",
    "    \n",
    "    pred_b = tf.keras.layers.Dense(1, activation='sigmoid', \n",
    "        name='pred_b')(enc_input_b)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=[text_input_a, text_input_b],\n",
    "        outputs=[pred_a, pred_b])\n",
    "    \n",
    "    return model \n",
    "    \n",
    "model = build_model()\n",
    "tf.keras.utils.plot_model(model, to_file='model.png')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 서브클래싱으로 모델링 \n",
    "\n",
    "* init, build, call, get_config, from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "1\n",
      "1\n",
      "0\n",
      "True\n",
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "            shape=(input_shape[1], self.output_dim), initializer='uniform', trainable=True)\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "        \n",
    "def test_my_layer():\n",
    "    \n",
    "    layer = MyLayer(output_dim=3)\n",
    "    y = layer(tf.ones([5, 4]))\n",
    "    \n",
    "    #print(y)\n",
    "    \n",
    "    print(layer.weights[0].shape)\n",
    "    print(len(layer.weights))\n",
    "    print(len(layer.trainable_weights))\n",
    "    print(len(layer.non_trainable_weights))  \n",
    "    print(layer.trainable)\n",
    "    print(layer.input_spec)\n",
    "    print(layer.losses)\n",
    "    \n",
    "test_my_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 서브 클래싱시에 add_loss 사용하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 가중치 저장/로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weight(/path/to/save)\n",
    "model.load_weights(/path/to/save)\n",
    "\n",
    "# to and from json\n",
    "json_string = model.to_json()    # or to_yaml\n",
    "model = tf.keras.models.model_from_json(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'activity_regularization_layer_6/mul:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "    \n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "model = keras.Model(inputs, outputs)    \n",
    "\n",
    "model.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 콜백 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(    callbacks=callbacks,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.tf.data.datasets 으로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'arc',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bccd',\n",
       " 'beans',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'blimp',\n",
       " 'bool_q',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'cherry_blossoms',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clic',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco_captions',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'coqa',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'dart',\n",
       " 'davis',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'drop',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'e2e_cleaned',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'genomics_ood',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'goemotions',\n",
       " 'gpt3',\n",
       " 'groove',\n",
       " 'gtzan',\n",
       " 'gtzan_music_speech',\n",
       " 'hellaswag',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'howell',\n",
       " 'i_naturalist2017',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_r',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lambada',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'lvis',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mctaco',\n",
       " 'mlqa',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'movielens',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'natural_questions_open',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'piqa',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'qa4mre',\n",
       " 'qasc',\n",
       " 'quac',\n",
       " 'quickdraw_bitmap',\n",
       " 'race',\n",
       " 'radon',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'robonet',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 's3o4d',\n",
       " 'salient_span_wikipedia',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'sentiment140',\n",
       " 'shapes3d',\n",
       " 'siscore',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'spoken_digit',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'story_cloze',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tydi_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vctk',\n",
       " 'vgg_face2',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_nlg',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_bio',\n",
       " 'wiki_table_questions',\n",
       " 'wiki_table_text',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'wine_quality',\n",
       " 'winogrande',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'wsc273',\n",
       " 'xnli',\n",
       " 'xquad',\n",
       " 'xsum',\n",
       " 'xtreme_pawsx',\n",
       " 'xtreme_xnli',\n",
       " 'yelp_polarity_reviews',\n",
       " 'yes_no']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n",
      "<TakeDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n",
      "['image', 'label']\n",
      "(28, 28, 1) tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "ds = tfds.load('mnist', split='train')\n",
    "print(ds)\n",
    "ds = ds.take(1)  # Only take a single example\n",
    "print(ds)\n",
    "\n",
    "# iterate하면 key를 통해 tensor에 접근한다.\n",
    "for example in ds:\n",
    "    print(list(example.keys()))\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"label\"]\n",
    "    print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# as_supervised를 사용시에는 데이터 세트 대신 튜플을 얻을 수 있다.\n",
    "\n",
    "ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "ds = ds.take(1)\n",
    "\n",
    "for image, label in ds:  # example is (image, label)\n",
    "    print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.int64'> 4\n"
     ]
    }
   ],
   "source": [
    "# tfds.as_numpy 를 사용 tfds.as_numpy 을 변환합니다.\n",
    "ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "ds = ds.take(1)\n",
    "\n",
    "for image, label in tfds.as_numpy(ds):\n",
    "    print(type(image), type(label), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >image</th>        <th class=\"col_heading level0 col1\" >label</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row0_col0\" class=\"data row0 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAzElEQVR4nGNgGPQg5F8qjMmEIRn1XwinRvnvp2QxdTIyMjAwMDDksd17jCnpeN6CgYGBQZfhAhbzLP+WMzAwyPz8IAkXQuh8ycDAwMAQyHr1ORZJYQYGBgYGKYYDDFgkAxgZGBikMxnnISQZYQz2J0KXjwvpqV00+YfpnsS/f//++/v3bxiSGAuMYfp97rN3b1cz7MDiEQgI+bcGmYsatlH/T+PUyPD2jwVOOaOP23Br3P3vZyZOO///v7qGARd4/EkBt7FvbuOWoyIAAPBxN9oBRuu9AAAAAElFTkSuQmCC\" alt=\"Img\" /></td>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row0_col1\" class=\"data row0 col1\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row1_col0\" class=\"data row1 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAmklEQVR4nGNgGOyAc/5KJlxyjIv+/TPEJan9798HWRgH3YhQBoZHj3HpfP/vVxQuOYF//54ieGjGNjEwXMalkeHbv3+eeCTfseAy1oCVYeofXBp3/f8lgUtO/su/azhtnPLvXwJOycv//uGU0//5bx1OySP//hngkuN5+u8tG4oIkj/VJBmO/8Il6cvAMBunlSIvX3DjlKQmAACHtTHZmy2LVAAAAABJRU5ErkJggg==\" alt=\"Img\" /></td>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row2_col0\" class=\"data row2 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+0lEQVR4nM3QvyuEARzH8feVniSFiTBcKf8A85ESNqwkdbeY5NdmY/UXiM0z+Ad0SXZ1g0vJjw0Xi86pc13eD8NleJ47q3zG76tv3x/wJxk+jQ7bf8NldT9obdNl1fWW1nGh6mtLPIm0oJVWlvuIvOvMWx1qtqWa0U2aULNNNnCtT+MQapi0waK6CoR6mcS8Wuhq4Fqj1PZjU5NQnX0DUine433dD1qZBwjOrM/EsVc9AmCj6c5MWT8XADjX2mgMi+otABN1k0/YVncAFu/VLWLbPgNBduRxLBMQHe/FZ+Zs5EtLm8kP9F81MHrZTScNelZK6sFcXzP9o3wDadaKxdoXqEQAAAAASUVORK5CYII=\" alt=\"Img\" /></td>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row3_col0\" class=\"data row3 col0\" ><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA40lEQVR4nGNgGJpg//8OdCEWKM2obvgv/++6/zcZnJQebPuNqkjvLxI4IIgiJ3/v79/3b//9/fvv7993f/9NQzE2TZ6hc8J3JwYGBgaGK7cYeJE12n79+1cCxlH9+/cVB5LOlxy/pryHSUYyMO78gSR5W/vzU7gxfAz/cfq36fvfDw445Fr+/P3bg12KMebb37+7WbDKKSz89/fvNRmscjpb//79u0YBq5z0yb9//2bhcEvXv78fs1ixyzX//PsuHYc+gbvwAMcEmX//3kFzJxOc9YCRoesJLp3sx9+p4JIjGQAAnrpmBs0pxioAAAAASUVORK5CYII=\" alt=\"Img\" /></td>\n",
       "                        <td id=\"T_3b6fc530_6305_11eb_b7c7_77b809cfe006row3_col1\" class=\"data row3 col1\" >7</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      4\n",
       "1  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      1\n",
       "2  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      0\n",
       "3  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      7"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as_dataframe: pandas dataframe으로 변환해서 얻을 수 있다.\n",
    "ds, info = tfds.load('mnist', split='train', with_info=True)\n",
    "\n",
    "tfds.as_dataframe(ds.take(4), info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAChCAYAAABDAe2JAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAI+UlEQVR4nO3dXWiU+RXH8XO6RteXoE0trTE0oquLgi8r2lZsN+lKodK1oI3QBNuLraiRrZYiC73NjS+9cG19QRRRKFokWGkraxWKLsUGNFXqqpQN6uqiksQXYlVQyemF0+3M/J9k5kyeZ5LMfD9XeY7nn5yLx59/5z95Rs1MAAD5+dJgDwAAwwmhCQAOhCYAOBCaAOBAaAKAwwhPs6py1I6AmelgzzAQ3NfoQ7eZfTW7yE4TAKJ9FlUkNAHAgdAEAAdCEwAcCE0AcCA0AcCB0AQAB0ITABwITQBwIDQBwIHQBAAHQhMAHAhNAHAgNAHAgdAEAAdCEwAcCE0AcCA0AcCB0AQAB0ITABwITQBwIDQBwIHQBAAH1+eeY+AaGhqC2tGjR4Pa2rVrg9q+ffsSmQnwGj16dMb17t27g54xY8YEtcbGxqDW29sb32BFwE4TABwITQBwIDQBwIHQBAAHDoKKrKmpKaiZWVCrqqoqxjhATqoa1Pbu3ZtxvWrVqry+1+bNm4PapUuXCpprsLDTBAAHQhMAHAhNAHDgNc2E1dbWZlwvXbo06Glvbw9qhw8fTmwmwGPWrFlBLZ/XMHt6eoLa/fv3Y5lpMLHTBAAHQhMAHAhNAHAgNAHAoWQPgqLekBsl6o3lcdqwYUPG9ciRI4Oe69evB7Xbt28nNhPgsXLlyoLW3bp1K6iVwn3NThMAHAhNAHAgNAHAgdAEAIeSPQiqr68Patu3bw9q69aty7hua2uLdY7Zs2fn7BluT3lBedm4cWPOnpcvXwa1qCcalQJ2mgDgQGgCgAOhCQAOhCYAOJTsQdCzZ8+CWtShTF1dXcb1QA6Campqcn7/x48fBz2HDh0q+GcCcZowYUJQGz9+fM51XV1dQe3IkSNxjDTksNMEAAdCEwAcCE0AcCA0AcChZA+COjs7i/4zly9fHtQqKioyri9cuBD03L17N7GZAI+WlpaC1l2+fDnmSYYudpoA4EBoAoADoQkADiX7mmZVVVXRf2Z1dXXOnjNnziQ/CFCg1atXF7Rux44dMU8ydLHTBAAHQhMAHAhNAHAgNAHAoWQPgqLeaJ7vZ6HnY/LkyUGtubk55888cOBAbDMAg+XRo0cZ16dPnx6cQQYBO00AcCA0AcCB0AQAB0ITABxK4iBo1KhRQW3NmjVBzcyCWmNjY8b1lClTgp6o3y6aM2dOUKusrAxqFy9ezLi+ceNG0AMMhnnz5gW17Kdy9WXXrl0Z11Gfe16q2GkCgAOhCQAOhCYAOBCaAOBQEgdBTU1NQS3fR8NlfxZ61AFP1AFSvrZs2ZJx3dvbW/D3AuK0bdu2oDZiRBgJL168CGrZB0HlhJ0mADgQmgDgQGgCgAOhCQAOJXEQtHDhwqD29OnToBb1WLY7d+5kXD948CDo6e7uDmqtra15zXby5Mm8+oAk1dbWBrVFixYFtahDz46OjqB27969eAYbhthpAoADoQkADoQmADiUxGua69evz6tWqIaGhqAW9dEZx44dC2o9PT2xzQEUatOmTUFt7Nixea2NehN8OWOnCQAOhCYAOBCaAOBAaAKAQ0kcBCUt6ilKUW8CPn/+fDHGAdzq6+sLXnvw4MHY5igF7DQBwIHQBAAHQhMAHAhNAHDgICgPdXV1QS3qIOjs2bPFGAfIae7cuRnXM2bMyGvd8ePHE5imtLDTBAAHQhMAHAhNAHAgNAHAgYOgLPPnzw9qUZ8FferUqaDW1taWyEyA186dOzOuKyoq8lrX0tKSxDglhZ0mADgQmgDgQGgCgAOvaWbZunVrUKusrAxqS5YsCWrNzc1Bbc+ePfEMBvRh3LhxQW3q1Kk51z18+DCoXb16NZaZShk7TQBwIDQBwIHQBAAHQhMAHDgIyhL19KKo2pUrV4Jaa2trIjMB/Yl6gtGkSZNyrjt37lxQe/78eSwzlTJ2mgDgQGgCgAOhCQAOhCYAOHAQlGXmzJlB7cmTJ0FtxYoVQa2rqyuRmYD+LFu2rKB1+/fvj3mS8sBOEwAcCE0AcCA0AcCB0AQAB436bZc+m1Xzbx6muru7g1rUI7SmT59ejHGGBTPTwZ5hIIb7fT1x4sSglv0ba1F/z6dNmxbUog49y1i7mS3ILrLTBAAHQhMAHAhNAHAgNAHAgYMgDBgHQShRHAQBwEARmgDgQGgCgAOhCQAOhCYAOBCaAOBAaAKAA6EJAA6EJgA4EJoA4EBoAoADoQkADoQmADh4P/e8W0Q+S2IQDFu1gz1ADLivESXy3nY9Gg4Ayh3/PQcAB0ITABwITQBwIDSzqOprqnpRVf/ST8+Hqvp2Vu23qvqftOv3VfW9JGcF8qGqB1S1U1U/ydH3S1X9Werrlap6RVV7VXVBWs9sVT2Y8MhDGqEZ2igi1/r6Q1X9ioh828w+TqstEJEvZ7UeEJFfJDIh4HNQRH7QX4OqjhCR90TkcKr0iYisEJGP0/vM7LKI1KjqN+Ifc3ggNNOoao2I/FBE9vfT9mMROZm25jUR+Y2IfJDeZGZPReSmqn4zgVGBvKX+gX+Qo+0dEfmnmb1MrblmZv/uo/fPIvKTGEccVgjNTB/Kq/Dr7adnsYi0p12/LyJ/MrO7Eb0XROS7sU0HJCf7vu5PWd/XhGaKqr4rIp1mluvGmSQiXak11SKyUkR+10dvp4hUxzYkkJwv7us8lPV9TWj+32IR+ZGq3hSRP4jIO6r6+4i+ZyLyeurrt0TkDRHpSK0bo6odab2vp/qBoS79vs6lrO9rQjPFzH5tZjVmNkVevV7zNzNbFdF6TV4FpZjZCTP7uplNSa17amZvpPXOkFcvqAND3Rf3dR7K+r4mNP1OiEh9nr2LReR0cqMAuanqERH5h4i8qaqfq+rPI9o+EpG309YsV9XPRWSRiJxQ1b+m9X5PXv09KEv87nkBVPXvIvKumT3qp+ctEfmVmf20aIMBA6CqfxSRD8zs0356RonIWRH5zv9O2ssNoVkAVf2WiDwzs3/10/N9EfnUzG4WbTBgAFT1TRH5Wvp7kCN6povIZDM7U7TBhhhCEwAceE0TABwITQBwIDQBwIHQBAAHQhMAHP4LT67iOINOTMYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show example의 사용 \n",
    "\n",
    "ds, info = tfds.load('mnist', split='train', with_info=True)\n",
    "#ds = ds.take(1)  # Only take a single example\n",
    "\n",
    "tfds.show_examples(ds.take(2), info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    full_name='mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    The MNIST database of handwritten digits.\n",
      "    \"\"\",\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    data_path='/home/hoondori/tensorflow_datasets/mnist/3.0.1',\n",
      "    download_size=11.06 MiB,\n",
      "    dataset_size=21.00 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 정보 표시\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선택적 직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units}\n",
    "\n",
    "\n",
    "# Now you can recreate the layer from its config:\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient tape로 자신만의 학습 fit 함수 만들기\n",
    "\n",
    "* Model sublcassing\n",
    "* train_step redefine\n",
    "* gradient tape에서 y, x 정의\n",
    "* 자신만의 loss 정의 (외부에서 줄 필요 없게 됨)\n",
    "* 자신만의 loss_tracker, metric 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_step 재정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.7846 - mae: 0.7633\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.3233 - mae: 0.4585\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2341 - mae: 0.3908\n"
     ]
    }
   ],
   "source": [
    "def simpleCustomModel():\n",
    "\n",
    "    class CustomModel(keras.Model):\n",
    "        def train_step(self, data):\n",
    "            # unpack\n",
    "            x, y = data\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                y_pred = self(x, training=True) # forward pass\n",
    "\n",
    "                loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "            # compute g\n",
    "            trainable_variables = self.trainable_variables\n",
    "            g = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "            # update w\n",
    "            self.optimizer.apply_gradients(zip(g, trainable_variables))\n",
    "            \n",
    "            # update metrics\n",
    "            self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "            # return dict of metrics\n",
    "            return { m.name: m.result() for m in self.metrics }     \n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Construct and compile an instance of CustomModel\n",
    "    inputs = keras.Input(shape=(32,))\n",
    "    outputs = keras.layers.Dense(1)(inputs)\n",
    "    model = CustomModel(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # Just use `fit` as usual\n",
    "    x = np.random.random((1000, 32))\n",
    "    y = np.random.random((1000, 1))\n",
    "    model.fit(x, y, epochs=3)     \n",
    "simpleCustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 낮은 수준 구현 \n",
    "\n",
    "* Model internal 손실과 Metric 정의\n",
    "  * 정의 손실과 Metric의 update state 을 손수 해준다.\n",
    "* 모델 컴파일시 이제는 exernal loss, metric을 지정해 주지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 738us/step - my_loss: 1.6299 - my_mae: 1.1668\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 755us/step - my_loss: 1.1200 - my_mae: 0.9225\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 753us/step - my_loss: 0.8444 - my_mae: 0.7652\n"
     ]
    }
   ],
   "source": [
    "def lowLevelCustomModel():\n",
    "\n",
    "    loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "    mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "    \n",
    "    class CustomModel(keras.Model):\n",
    "        def train_step(self, data):\n",
    "            # unpack\n",
    "            x, y = data\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                y_pred = self(x, training=True) # forward pass\n",
    "\n",
    "                # use model's internal loss\n",
    "                loss = keras.losses.mean_squared_error(y, y_pred)\n",
    "\n",
    "            # compute g\n",
    "            trainable_variables = self.trainable_variables\n",
    "            g = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "            # update w\n",
    "            self.optimizer.apply_gradients(zip(g, trainable_variables))\n",
    "            \n",
    "            # compute our own metrics\n",
    "            loss_tracker.update_state(loss)\n",
    "            mae_metric.update_state(y, y_pred)\n",
    "\n",
    "            # return dict of metrics\n",
    "            return { \"my_loss\": loss_tracker.result(), \"my_mae\": mae_metric.result() }     \n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Construct and compile an instance of CustomModel\n",
    "    inputs = keras.Input(shape=(32,))\n",
    "    outputs = keras.layers.Dense(1)(inputs)\n",
    "    model = CustomModel(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\") #, loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # Just use `fit` as usual\n",
    "    x = np.random.random((1000, 32))\n",
    "    y = np.random.random((1000, 1))\n",
    "    model.fit(x, y, epochs=3)     \n",
    "    \n",
    "lowLevelCustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample weight 및  class weight  지원\n",
    "\n",
    "* data에서 sw 전달\n",
    "* unpack data\n",
    "* compiled_loss에 sw, cw 전달\n",
    "* metric update_state 시에도 sw 전달 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32/32 [==============================] - 0s 783us/step - loss: 2.2923 - mae: 2.0717\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 0s 769us/step - loss: 1.3720 - mae: 1.5699\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.7871 - mae: 1.1423\n"
     ]
    }
   ],
   "source": [
    "def SampleWeightCustomModel():\n",
    "\n",
    "    class CustomModel(keras.Model):\n",
    "        def train_step(self, data):\n",
    "            # unpack\n",
    "            x, y, sw = data\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                y_pred = self(x, training=True) # forward pass\n",
    "\n",
    "                loss = self.compiled_loss(y, y_pred, \n",
    "                    sample_weight=sw,                      \n",
    "                    regularization_losses=self.losses)\n",
    "\n",
    "            # compute g\n",
    "            trainable_variables = self.trainable_variables\n",
    "            g = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "            # update w\n",
    "            self.optimizer.apply_gradients(zip(g, trainable_variables))\n",
    "            \n",
    "            # update metrics\n",
    "            self.compiled_metrics.update_state(y, y_pred, sample_weight=sw)\n",
    "\n",
    "            # return dict of metrics\n",
    "            return { m.name: m.result() for m in self.metrics }     \n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Construct and compile an instance of CustomModel\n",
    "    inputs = keras.Input(shape=(32,))\n",
    "    outputs = keras.layers.Dense(1)(inputs)\n",
    "    model = CustomModel(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # You can now use sample_weight argument\n",
    "    x = np.random.random((1000, 32))\n",
    "    y = np.random.random((1000, 1))\n",
    "    sw = np.random.random((1000, 1))\n",
    "    model.fit(x, y, sample_weight=sw, epochs=3)\n",
    "    \n",
    "SampleWeightCustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자신의 평가 단계 제공 (model.evaluate 재작성)\n",
    "\n",
    "* test_step 재정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 643us/step - loss: 0.2098 - mae: 0.3705\n"
     ]
    }
   ],
   "source": [
    "def MyOwnEvaluationCustomModel():\n",
    "\n",
    "    class CustomModel(keras.Model):\n",
    "        def test_step(self, data):\n",
    "            # Unpack the data\n",
    "            x, y = data\n",
    "            # Compute predictions\n",
    "            y_pred = self(x, training=False)\n",
    "            # Updates the metrics tracking the loss\n",
    "            self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            # Update the metrics.\n",
    "            self.compiled_metrics.update_state(y, y_pred)\n",
    "            # Return a dict mapping metric names to current value.\n",
    "            # Note that it will include the loss (tracked in self.metrics).\n",
    "            return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    # Construct an instance of CustomModel\n",
    "    inputs = keras.Input(shape=(32,))\n",
    "    outputs = keras.layers.Dense(1)(inputs)\n",
    "    model = CustomModel(inputs, outputs)\n",
    "    model.compile(loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # Evaluate with our custom test_step\n",
    "    x = np.random.random((1000, 32))\n",
    "    y = np.random.random((1000, 1))\n",
    "    model.evaluate(x, y)\n",
    "    \n",
    "MyOwnEvaluationCustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마무리 : EndToEnd GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     11,
     46
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 2s 16ms/step - d_loss: 0.5195 - g_loss: 0.8054\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 2s 16ms/step - d_loss: 0.3302 - g_loss: 1.3691\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 2s 16ms/step - d_loss: 0.5003 - g_loss: 1.3242\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow.keras import datasets, layers, models, preprocessing\n",
    "from tensorflow.python.client import device_lib\n",
    "import timeit\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def SimpleGAN(epochs=1):\n",
    "    \n",
    "    def build_model(latent_dim):\n",
    "    \n",
    "        # create discriminator\n",
    "        discriminator = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(28, 28, 1)),\n",
    "                layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.GlobalMaxPooling2D(),\n",
    "                layers.Dense(1),\n",
    "            ],\n",
    "            name=\"discriminator\",\n",
    "        )\n",
    "\n",
    "        # Create the generator\n",
    "        generator = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(latent_dim,)),\n",
    "                # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
    "                layers.Dense(7 * 7 * 128),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Reshape((7, 7, 128)),\n",
    "                layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                layers.LeakyReLU(alpha=0.2),\n",
    "                layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "            ],\n",
    "            name=\"generator\",\n",
    "        )\n",
    "        \n",
    "        return discriminator, generator\n",
    "    \n",
    "    class GAN(keras.Model):\n",
    "        def __init__(self, discriminator, generator, latent_dim):\n",
    "            super(GAN, self).__init__()\n",
    "            self.discriminator = discriminator\n",
    "            self.generator = generator\n",
    "            self.latent_dim = latent_dim\n",
    "        \n",
    "        \n",
    "        def compile(self, d_opt, g_opt, loss_fn):\n",
    "            super(GAN, self).compile()\n",
    "            self.d_opt = d_opt\n",
    "            self.g_opt = g_opt\n",
    "            self.loss_fn = loss_fn\n",
    "        \n",
    "        def train_step(self, real_images):\n",
    "            \n",
    "            if isinstance(real_images, tuple):\n",
    "                real_images = real_imaages[0]\n",
    "                \n",
    "            # sampl random points in latent space\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "            # fake image generation by generator\n",
    "            gen_images = self.generator(random_latent_vectors)\n",
    "            \n",
    "            \n",
    "            #### train the discriminator\n",
    "            \n",
    "            # combine real/fake images\n",
    "            combined_images = tf.concat([gen_images, real_images], axis=0)\n",
    "            # prepare labels (fake -> 1, real->0)\n",
    "            labels = tf.concat([tf.ones(batch_size,1), tf.zeros(batch_size,1)], axis=0)\n",
    "            # add label noise \n",
    "            labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = self.discriminator(combined_images)\n",
    "                d_loss = self.loss_fn(labels, preds)\n",
    "            grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "            self.d_opt.apply_gradients(\n",
    "                zip(grads, self.discriminator.trainable_weights)\n",
    "            )\n",
    "            \n",
    "            ### train the generator\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            misleading_labels = tf.zeros((batch_size, 1))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = self.discriminator(self.generator(random_latent_vectors))\n",
    "                g_loss = self.loss_fn(misleading_labels, preds)\n",
    "            grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "            self.g_opt.apply_gradients(\n",
    "                zip(grads, self.generator.trainable_weights)\n",
    "            )\n",
    "            \n",
    "            return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "        \n",
    "    # Prepare the dataset. We use both the training & test MNIST digits.\n",
    "    batch_size = 64   \n",
    "    \n",
    "    (X_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "    all_digits = np.concatenate([X_train, x_test])\n",
    "    all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "    all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    # model\n",
    "    latent_dim = 128\n",
    "    discriminator, generator = build_model(latent_dim=latent_dim)\n",
    "    \n",
    "    gan = GAN(discriminator, generator, latent_dim=latent_dim)\n",
    "    gan.compile(\n",
    "        d_opt=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "        g_opt=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "        loss_fn=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    )\n",
    "    \n",
    "    gan.fit(dataset.take(100), epochs=epochs)\n",
    "            \n",
    "SimpleGAN(epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "291px",
    "width": "356px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
