{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 로 양자화 인식 훈련 with MNIST\n",
    "\n",
    "* https://www.tensorflow.org/model_optimization/guide/quantization/training_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU[0] is ready\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU[0] is ready')\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)\n",
    "else:\n",
    "    print('Please check GPU available')\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # suppress warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT 없이 MNIST 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 12)        120       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20290     \n",
      "=================================================================\n",
      "Total params: 20,410\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture.\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 900us/step - loss: 0.3208 - accuracy: 0.9086 - val_loss: 0.1561 - val_accuracy: 0.9570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7eff053822b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=1,\n",
    "  validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT로 pre-trained 모델 복제 및 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_2 (QuantizeLa (None, 28, 28)            3         \n",
      "_________________________________________________________________\n",
      "quant_reshape_2 (QuantizeWra (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_2 (QuantizeWrap (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_2 (Quant (None, 13, 13, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten_2 (QuantizeWra (None, 2028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense_2 (QuantizeWrapp (None, 10)                20295     \n",
      "=================================================================\n",
      "Total params: 20,448\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 38\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# quantize model requires a recompile\n",
    "q_aware_model.compile(optimizer='adam', \n",
    "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1933 - accuracy: 0.9456 - val_loss: 0.2094 - val_accuracy: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7eff05077780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 미세 조정을 위해 subset of input data 선정 후 fitting\n",
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline 과 q-aware finetuned 모델과의 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9495000243186951\n",
      "Quant test accuracy: 0.9535999894142151\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 가중치 및  UINT8 활성화를 통한 양자화된 모델 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpb1cvs8df/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpb1cvs8df/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()  # byte array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Lite 모델 성능 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.9536\n",
      "Quant TF test accuracy: 0.9535999894142151\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    print('\\n')\n",
    "        \n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized model(정수) 는 Float(32) TF Lite 보다 4배 작아진 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpow03y2a1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpow03y2a1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.0806121826171875\n",
      "Quantized model in Mb: 0.02344512939453125\n"
     ]
    }
   ],
   "source": [
    "# Create float TFLite model.\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "float_tflite_model = float_converter.convert()\n",
    "\n",
    "# Measure sizes of models.\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "    f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
