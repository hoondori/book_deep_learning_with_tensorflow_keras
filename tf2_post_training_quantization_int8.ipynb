{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 후 정수 양자화\n",
    "\n",
    "* https://www.tensorflow.org/lite/performance/post_training_integer_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU[0] is ready\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU[0] is ready')\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)\n",
    "else:\n",
    "    print('Please check GPU available')\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "assert float(tf.__version__[:3]) >= 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반 학습으로 MNIST 모델 생성 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 867us/step - loss: 0.3149 - accuracy: 0.9086 - val_loss: 0.1917 - val_accuracy: 0.9448\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 822us/step - loss: 0.1556 - accuracy: 0.9546 - val_loss: 0.1114 - val_accuracy: 0.9687\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 827us/step - loss: 0.1014 - accuracy: 0.9707 - val_loss: 0.0826 - val_accuracy: 0.9748\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 837us/step - loss: 0.0774 - accuracy: 0.9776 - val_loss: 0.0702 - val_accuracy: 0.9784\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 837us/step - loss: 0.0636 - accuracy: 0.9812 - val_loss: 0.0640 - val_accuracy: 0.9789\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 851us/step - loss: 0.0557 - accuracy: 0.9834 - val_loss: 0.0580 - val_accuracy: 0.9815\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 836us/step - loss: 0.0501 - accuracy: 0.9845 - val_loss: 0.0606 - val_accuracy: 0.9800\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 834us/step - loss: 0.0445 - accuracy: 0.9872 - val_loss: 0.0558 - val_accuracy: 0.9812\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 832us/step - loss: 0.0404 - accuracy: 0.9879 - val_loss: 0.0606 - val_accuracy: 0.9815\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 834us/step - loss: 0.0369 - accuracy: 0.9893 - val_loss: 0.0561 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb65bd3c0b8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images, test_images = train_images.astype(np.float32)/255., test_images.astype(np.float32)/255.\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=10,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite 모델로 변환하기  (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/logs/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6071f8uc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6071f8uc/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84528"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 양자화된 모델로 저장하기, 완전 정수화 모델  => 1/4 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuyz6l8vu/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuyz6l8vu/assets\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24784"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the quantized model:\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter_quant.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dtype: <class 'numpy.uint8'>, output dtype: <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "# 입출력은 UINT8\n",
    "in_dtype = interpreter_quant.get_input_details()[0]['dtype']\n",
    "out_dtype = interpreter_quant.get_output_details()[0]['dtype']\n",
    "print(f'input dtype: {in_dtype}, output dtype: {out_dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scales': array([0.00392157], dtype=float32),\n",
       " 'zero_points': array([0], dtype=int32),\n",
       " 'quantized_dimension': 0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter_quant.get_input_details()[0]['quantization_parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scales': array([0.21143739], dtype=float32),\n",
       " 'zero_points': array([180], dtype=int32),\n",
       " 'quantized_dimension': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter_quant.get_output_details()[0]['quantization_parameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수형 입력을 사용하여 TensorFlow Lite 모델 예측 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image shape: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "print(f'test image shape: {test_images[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "    global test_images\n",
    "\n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "    for i, test_image_index in enumerate(test_image_indices):\n",
    "        test_image = test_images[test_image_index]\n",
    "        test_label = test_labels[test_image_index]\n",
    "\n",
    "        # Check if the input type is quantized, then rescale input data to uint8\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "        # Check if the output type is quantized, then rescale output data to float\n",
    "        if output_details['dtype'] == np.uint8:\n",
    "            output_scale, output_zero_point = output_details[\"quantization\"]\n",
    "            test_image = test_image.astype(np.float32)\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        predictions[i] = output.argmax()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Change this to test a different image\n",
    "test_image_index = 9874\n",
    "\n",
    "## Helper function to test the models on one image\n",
    "def test_model(tflite_file, test_image_index, model_type):\n",
    "    global test_labels\n",
    "\n",
    "    predictions = run_tflite_model(tflite_file, [test_image_index])\n",
    "\n",
    "    plt.imshow(test_images[test_image_index])\n",
    "    template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
    "    _ = plt.title(template.format(true= str(test_labels[test_image_index]), predict=str(predictions[0])))\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## F32 모델의 경우 \n",
    "test_model(tflite_model_quant_file, test_image_index, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 정수형 모델의 경우\n",
    "test_model(tflite_model_file, test_image_index, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모든 파일 예측해서 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "    global test_images\n",
    "    global test_labels\n",
    "   \n",
    "    test_image_indices = range(test_images.shape[0])\n",
    "    predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "      \n",
    "    accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
    "\n",
    "    print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "        model_type, accuracy, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 98.2300% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_file, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 98.1800% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
