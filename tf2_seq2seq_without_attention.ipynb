{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 로 seq2seq 구현  (no attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY\n",
    "%load_ext tensorboard\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import unicodedata\n",
    "import zipfile\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 처리\n",
    "\n",
    "* teacher forcing을 사용해야 하므로 (X, Y, Y') 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000 30000\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \n",
    "        if unicodedata.category(c) != \"Mn\"])\n",
    "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    sent = sent.lower()\n",
    "    return sent\n",
    "\n",
    "\n",
    "def download_and_read():\n",
    "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
    "    local_file = '/home/hoondori/data/fra-eng/fra.txt'\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            en_sent, fr_sent, _ = line.strip().split('\\t')\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "            fr_sent = preprocess_sentence(fr_sent)\n",
    "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
    "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n",
    "            en_sents.append(en_sent)\n",
    "            fr_sents_in.append(fr_sent_in)\n",
    "            fr_sents_out.append(fr_sent_out)\n",
    "            if i >= NUM_SENT_PAIRS - 1:\n",
    "                break\n",
    "    return en_sents, fr_sents_in, fr_sents_out\n",
    "\n",
    "NUM_SENT_PAIRS = 30000\n",
    "sents_en, sents_fr_in, sents_fr_out = download_and_read()\n",
    "print(len(sents_en), len(sents_fr_in), len(sents_fr_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (en): 4339, vocab size (fr): 7649\n",
      "seqlen (en): 8, (fr): 16\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en)\n",
    "data_en = tokenizer_en.texts_to_sequences(sents_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")\n",
    "\n",
    "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
    "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\n",
    "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\n",
    "\n",
    "vocab_size_en = len(tokenizer_en.word_index)\n",
    "vocab_size_fr = len(tokenizer_fr.word_index)\n",
    "word2idx_en = tokenizer_en.word_index\n",
    "idx2word_en = {v:k for k, v in word2idx_en.items()}\n",
    "word2idx_fr = tokenizer_fr.word_index\n",
    "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\n",
    "print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\n",
    "    vocab_size_en, vocab_size_fr))\n",
    "\n",
    "maxlen_en = data_en.shape[1]\n",
    "maxlen_fr = data_fr_out.shape[1]\n",
    "print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 8), (64, 16), (64, 16)), types: (tf.int32, tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "BATCH_SIZE = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = NUM_SENT_PAIRS // 4\n",
    "test_dataset = dataset.take(test_size).batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps, encoder_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            encoder_dim, return_sequences=False, return_state=True)\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, initial_state=state)\n",
    "        return x, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps,decoder_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            decoder_dim, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, state)\n",
    "        x = self.dense(x)\n",
    "        return x, state\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "ENCODER_DIM, DECODER_DIM = 1024, 1024    \n",
    "\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "encoder_dim, decoder_dim = ENCODER_DIM, DECODER_DIM\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\n",
    "decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input : (64, 8)\n",
      "encoder state : (64, 1024)\n",
      "decoder input : (64, 16)\n",
      "decoder predict : (64, 16, 7650)\n",
      "decoder out(label) : (64, 16)\n"
     ]
    }
   ],
   "source": [
    "# 디버깅 (출력 점검)\n",
    "\n",
    "for enc_in, dec_in, dec_out in train_dataset.take(1):\n",
    "    enc_init_state = encoder.init_state(BATCH_SIZE)\n",
    "    _,enc_state = encoder(enc_in, enc_init_state)\n",
    "    \n",
    "    dec_predict, _ = decoder(dec_in, enc_state)\n",
    "    \n",
    "    print(\"encoder input :\", enc_in.shape)   # (batch_size, maxlen_en)\n",
    "    print(\"encoder state :\", enc_state.shape) # (batch_size, encdoer_dim)\n",
    "\n",
    "    print(\"decoder input :\", dec_in.shape) # (batch_size, decoder_dim)\n",
    "    print(\"decoder predict :\", dec_predict.shape) # (batch_size, maxlen_fr, vocab_size_fr+1)\n",
    "    print(\"decoder out(label) :\", dec_out.shape) # (batch_size, maxlen_fr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 입력 sentence에 대해서 출력 문자 만들기\n",
    "def predict(encoder, decoder, batch_size, \n",
    "        sents_en, data_en, sents_fr_out, \n",
    "        word2idx_fr, idx2word_fr):\n",
    "    random_id = np.random.choice(len(sents_en))\n",
    "    print(\"input    : \",  \" \".join(sents_en[random_id]))\n",
    "    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n",
    "\n",
    "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
    "    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n",
    "\n",
    "    encoder_state = encoder.init_state(1)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    decoder_in = tf.expand_dims(\n",
    "        tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\n",
    "    pred_sent_fr = []\n",
    "    while True:\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
    "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n",
    "        pred_sent_fr.append(pred_word)\n",
    "        if pred_word == \"EOS\":\n",
    "            break\n",
    "        decoder_in = decoder_pred\n",
    "    \n",
    "    print(\"predicted: \", \" \".join(pred_sent_fr))\n",
    "\n",
    "\n",
    "def evaluate_bleu_score(encoder, decoder, test_dataset, \n",
    "        word2idx_fr, idx2word_fr):\n",
    "\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction()\n",
    "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
    "        encoder_state = encoder.init_state(batch_size)\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "\n",
    "        # compute argmax\n",
    "        decoder_out = decoder_out.numpy()\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n",
    "\n",
    "        for i in range(decoder_out.shape[0]):\n",
    "            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\n",
    "            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\n",
    "            # remove trailing EOS\n",
    "            ref_sent = ref_sent[0:-1]\n",
    "            hyp_sent = hyp_sent[0:-1]\n",
    "            bleu_score = sentence_bleu([ref_sent], hyp_sent, \n",
    "                smoothing_function=smooth_fn.method1)\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "    return np.mean(np.array(bleu_scores))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 정의 : 둘 다 zero(PAD)인 것은 제외하고 평가\n",
    "\n",
    "def loss_fn(ytrue, ypred):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher forcing을 적용한 train_step 재정의\n",
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        loss = loss_fn(decoder_out, decoder_pred)\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/tmp/logs/seq2seq/'\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "num_epochs = 270\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.1619\n",
      "input    :  i need this one .\n",
      "label    :  il me faut celui ci . EOS\n",
      "predicted:  je ne suis pas pas . EOS\n",
      "Eval Score (BLEU): 2.252e-02\n",
      "Epoch: 2, Loss: 0.9223\n",
      "input    :  now is your chance .\n",
      "label    :  voila ta chance . EOS\n",
      "predicted:  c est ton heure . EOS\n",
      "Eval Score (BLEU): 3.217e-02\n",
      "Epoch: 3, Loss: 0.6904\n",
      "input    :  who are we ?\n",
      "label    :  qui sommes nous ? EOS\n",
      "predicted:  qui sont ils ? EOS\n",
      "Eval Score (BLEU): 4.349e-02\n",
      "Epoch: 4, Loss: 0.5335\n",
      "input    :  won t you go ?\n",
      "label    :  ne vas tu pas y aller ? EOS\n",
      "predicted:  n avez vous pas informe ? EOS\n",
      "Eval Score (BLEU): 5.596e-02\n",
      "Epoch: 5, Loss: 0.3601\n",
      "input    :  why not try it on ?\n",
      "label    :  pourquoi ne pas l essayer ? EOS\n",
      "predicted:  pourquoi ne pas l essayer ? EOS\n",
      "Eval Score (BLEU): 7.211e-02\n",
      "Epoch: 6, Loss: 0.2946\n",
      "input    :  i don t work here .\n",
      "label    :  je ne travaille pas ici . EOS\n",
      "predicted:  je ne travaille pas ici . EOS\n",
      "Eval Score (BLEU): 8.547e-02\n",
      "Epoch: 7, Loss: 0.3112\n",
      "input    :  let me try that .\n",
      "label    :  laisse moi essayer ca . EOS\n",
      "predicted:  laisse moi essayer ca . EOS\n",
      "Eval Score (BLEU): 9.823e-02\n",
      "Epoch: 8, Loss: 0.2491\n",
      "input    :  we need to win .\n",
      "label    :  il nous faut gagner . EOS\n",
      "predicted:  il nous faut l emporter . EOS\n",
      "Eval Score (BLEU): 1.147e-01\n",
      "Epoch: 9, Loss: 0.2048\n",
      "input    :  no one s working .\n",
      "label    :  personne ne travaille . EOS\n",
      "predicted:  personne ne travaille . EOS\n",
      "Eval Score (BLEU): 1.237e-01\n",
      "Epoch: 10, Loss: 0.2350\n",
      "input    :  drink up your milk .\n",
      "label    :  bois ton lait . EOS\n",
      "predicted:  finis ton lait ! EOS\n",
      "Eval Score (BLEU): 1.281e-01\n",
      "Epoch: 11, Loss: 0.1280\n",
      "input    :  tom attacked me .\n",
      "label    :  tom m a agresse . EOS\n",
      "predicted:  tom m a attaque . EOS\n",
      "Eval Score (BLEU): 1.354e-01\n",
      "Epoch: 12, Loss: 0.1776\n",
      "input    :  i want a divorce .\n",
      "label    :  je veux divorcer . EOS\n",
      "predicted:  je veux divorcer . EOS\n",
      "Eval Score (BLEU): 1.392e-01\n",
      "Epoch: 13, Loss: 0.1568\n",
      "input    :  she got up late .\n",
      "label    :  elle s est levee tard . EOS\n",
      "predicted:  elle s est levee tard . EOS\n",
      "Eval Score (BLEU): 1.419e-01\n",
      "Epoch: 14, Loss: 0.1860\n",
      "input    :  do it this way .\n",
      "label    :  faites le de cette maniere . EOS\n",
      "predicted:  faites le ainsi . EOS\n",
      "Eval Score (BLEU): 1.433e-01\n",
      "Epoch: 15, Loss: 0.1179\n",
      "input    :  they all entered .\n",
      "label    :  elles entrerent toutes . EOS\n",
      "predicted:  elles sont toutes entrees . EOS\n",
      "Eval Score (BLEU): 1.475e-01\n",
      "Epoch: 16, Loss: 0.1349\n",
      "input    :  i don t know yet .\n",
      "label    :  je ne sais pas encore . EOS\n",
      "predicted:  je ne sais pas encore . EOS\n",
      "Eval Score (BLEU): 1.469e-01\n",
      "Epoch: 17, Loss: 0.1502\n",
      "input    :  is this your beer ?\n",
      "label    :  est ce votre biere ? EOS\n",
      "predicted:  s agit il de votre biere ? EOS\n",
      "Eval Score (BLEU): 1.511e-01\n",
      "Epoch: 18, Loss: 0.1347\n",
      "input    :  it s my umbrella .\n",
      "label    :  c est mon parapluie . EOS\n",
      "predicted:  c est mon parapluie . EOS\n",
      "Eval Score (BLEU): 1.507e-01\n",
      "Epoch: 19, Loss: 0.1289\n",
      "input    :  tom looks excited .\n",
      "label    :  tom a l air enthousiaste . EOS\n",
      "predicted:  tom a l air enthousiaste . EOS\n",
      "Eval Score (BLEU): 1.512e-01\n",
      "Epoch: 20, Loss: 0.1212\n",
      "input    :  who is that man ?\n",
      "label    :  qui est cet homme ? EOS\n",
      "predicted:  qui est cet homme ? EOS\n",
      "Eval Score (BLEU): 1.529e-01\n",
      "Epoch: 21, Loss: 0.1263\n",
      "input    :  everyone s asleep .\n",
      "label    :  tout le monde dort . EOS\n",
      "predicted:  tout le monde dort . EOS\n",
      "Eval Score (BLEU): 1.555e-01\n",
      "Epoch: 22, Loss: 0.1219\n",
      "input    :  be prepared .\n",
      "label    :  soyez preparees ! EOS\n",
      "predicted:  sois preparee ! EOS\n",
      "Eval Score (BLEU): 1.555e-01\n",
      "Epoch: 23, Loss: 0.1183\n",
      "input    :  grab that .\n",
      "label    :  saisis toi de ca ! EOS\n",
      "predicted:  attrape ca ! EOS\n",
      "Eval Score (BLEU): 1.547e-01\n",
      "Epoch: 24, Loss: 0.1408\n",
      "input    :  you re awesome .\n",
      "label    :  tu es genial . EOS\n",
      "predicted:  tu es geniale . EOS\n",
      "Eval Score (BLEU): 1.565e-01\n",
      "Epoch: 25, Loss: 0.1255\n",
      "input    :  is it yours ?\n",
      "label    :  est ce le tien ? EOS\n",
      "predicted:  est ce le tien ? EOS\n",
      "Eval Score (BLEU): 1.572e-01\n",
      "Epoch: 26, Loss: 0.1347\n",
      "input    :  you sound tired .\n",
      "label    :  vous paraissez fatiguees . EOS\n",
      "predicted:  vous paraissez fatiguee . EOS\n",
      "Eval Score (BLEU): 1.565e-01\n",
      "Epoch: 27, Loss: 0.1380\n",
      "input    :  i came yesterday .\n",
      "label    :  je suis venu hier . EOS\n",
      "predicted:  je suis venu hier . EOS\n",
      "Eval Score (BLEU): 1.568e-01\n",
      "Epoch: 28, Loss: 0.1113\n",
      "input    :  don t leave .\n",
      "label    :  ne partez pas ! EOS\n",
      "predicted:  ne partez pas ! EOS\n",
      "Eval Score (BLEU): 1.579e-01\n",
      "Epoch: 29, Loss: 0.1091\n",
      "input    :  they re great .\n",
      "label    :  elles sont formidables . EOS\n",
      "predicted:  ils sont geniaux . EOS\n",
      "Eval Score (BLEU): 1.582e-01\n",
      "Epoch: 30, Loss: 0.1231\n",
      "input    :  you re safe here .\n",
      "label    :  tu es ici en securite . EOS\n",
      "predicted:  vous etes ici en securite . EOS\n",
      "Eval Score (BLEU): 1.601e-01\n",
      "Epoch: 31, Loss: 0.1017\n",
      "input    :  i stole a gun .\n",
      "label    :  j ai vole une arme a feu . EOS\n",
      "predicted:  j ai vole un flingue . EOS\n",
      "Eval Score (BLEU): 1.568e-01\n",
      "Epoch: 32, Loss: 0.1236\n",
      "input    :  i want to cry .\n",
      "label    :  j ai envie de pleurer . EOS\n",
      "predicted:  j ai envie de pleurer . EOS\n",
      "Eval Score (BLEU): 1.602e-01\n",
      "Epoch: 33, Loss: 0.1504\n",
      "input    :  tom was trapped .\n",
      "label    :  tom a ete piege . EOS\n",
      "predicted:  tom a ete piege . EOS\n",
      "Eval Score (BLEU): 1.593e-01\n",
      "Epoch: 34, Loss: 0.1158\n",
      "input    :  he ll get over it .\n",
      "label    :  il s en remettra . EOS\n",
      "predicted:  il s en remettra . EOS\n",
      "Eval Score (BLEU): 1.595e-01\n",
      "Epoch: 35, Loss: 0.1026\n",
      "input    :  i don t like it .\n",
      "label    :  ca ne me plait pas . EOS\n",
      "predicted:  je n aime pas ca . EOS\n",
      "Eval Score (BLEU): 1.578e-01\n",
      "Epoch: 36, Loss: 0.1196\n",
      "input    :  please try one .\n",
      "label    :  essayez en une . EOS\n",
      "predicted:  essayez en une . EOS\n",
      "Eval Score (BLEU): 1.585e-01\n",
      "Epoch: 37, Loss: 0.1062\n",
      "input    :  i want a job .\n",
      "label    :  je veux un boulot . EOS\n",
      "predicted:  je veux un boulot . EOS\n",
      "Eval Score (BLEU): 1.605e-01\n",
      "Epoch: 38, Loss: 0.1099\n",
      "input    :  i ll pack my bag .\n",
      "label    :  je ferai mon sac . EOS\n",
      "predicted:  je ferai mon sac . EOS\n",
      "Eval Score (BLEU): 1.574e-01\n",
      "Epoch: 39, Loss: 0.1284\n",
      "input    :  it s an old pain .\n",
      "label    :  c est une vieille douleur . EOS\n",
      "predicted:  c est une vieille douleur . EOS\n",
      "Eval Score (BLEU): 1.595e-01\n",
      "Epoch: 40, Loss: 0.1137\n",
      "input    :  are they dead ?\n",
      "label    :  sont ils morts ? EOS\n",
      "predicted:  sont elles mortes ? EOS\n"
     ]
    }
   ],
   "source": [
    "eval_scores = []\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "\n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "        loss = train_step(\n",
    "            encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "    \n",
    "    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    predict(encoder, decoder, batch_size, sents_en, data_en,\n",
    "        sents_fr_out, word2idx_fr, idx2word_fr)\n",
    "\n",
    "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n",
    "    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n",
    "    # eval_scores.append(eval_score)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
