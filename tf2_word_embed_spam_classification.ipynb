{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드 임베딩을 사용한 스펨 탐지\n",
    "\n",
    "* scratch로부터 word embedding을 학습시키는 것과 GloVe와 같은 pre-trained embedding을 사용하는 것과 이를 fine-tuning하는 것을 상호-비교해 본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY\n",
    "%load_ext tensorboard\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\"/tmp\")\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"/tmp\", \"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == \"spam\" else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "# read data\n",
    "texts, labels = download_and_read(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\")\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리/데이터셋화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 sentences, max length: 189, voca: 9010\n",
      "text=Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "text sequence=[ 123  351 1328  148 2996 1329   67   58 4436  144]...\n"
     ]
    }
   ],
   "source": [
    "def createTextSequences(texts, labels):\n",
    "    \n",
    "    # tokenizer fitting on texts\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None) # 지정 없으면 전수 단어 사용하여 integer 표현으로 변경\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # text sequence 생성\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "    num_records = len(text_sequences)\n",
    "    max_seqlen = len(text_sequences[0])\n",
    "    \n",
    "    # 어휘 \n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = {v:k for k,v in word2idx.items()}\n",
    "    word2idx[\"PAD\"] = 0\n",
    "    idx2word[0] = \"PAD\"\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "    print(\"{:d} sentences, max length: {:d}, voca: {:d}\".format(num_records, max_seqlen, vocab_size))\n",
    "      \n",
    "    return text_sequences, word2idx, idx2word, vocab_size, max_seqlen, tokenizer\n",
    "\n",
    "text_sequences, word2idx, idx2word, vocab_size, max_seqlen, tokenizer = createTextSequences(texts, labels)\n",
    "print(f'text={texts[0]}')\n",
    "print(f'text sequence={text_sequences[0][-10:]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 189), (128, 2)), types: (tf.int32, tf.float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createTextSequences(text_sequences, labels, num_classes, batch_size):\n",
    "    cat_labels = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "    dataset = dataset.shuffle(10000)\n",
    "    \n",
    "    num_records = len(text_sequences)\n",
    "    test_size = num_records // 4   # 25%\n",
    "    val_size = (num_records - test_size) // 10 \n",
    "    \n",
    "    test_dataset = dataset.take(test_size)\n",
    "    val_dataset = dataset.skip(test_size).take(val_size)\n",
    "    train_dataset = dataset.skip(test_size+val_size)\n",
    "    \n",
    "    test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
    "    val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dataset, val_dataset, test_dataset = createTextSequences(text_sequences, labels, num_classes=2, batch_size=BATCH_SIZE)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix:  (9010, 300)\n"
     ]
    }
   ],
   "source": [
    "# gensim에서 제공하는 300차원 GloVe 임베딩을 사용해서 변환\n",
    "#   sequence of encoded integers => sequence of embeded vector\n",
    "\n",
    "def build_embedding_matrix(text_sequences, word2idx, embedding_file):\n",
    "    \n",
    "    # E : dict of word_index to embedding vector\n",
    "    \n",
    "    # 기존 것이 있으면 로딩해서 사용\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        # 기존 것이 없으므로 gensim에서 다운받아 구축\n",
    "        word_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
    "        \n",
    "        # container 준비\n",
    "        E = np.zeros((len(word2idx), 300))\n",
    "        \n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)  # 없으면 어떻게?\n",
    "            except:\n",
    "                # 없는 경우...\n",
    "                pass\n",
    "        \n",
    "        # 향후 사용을 위해 저장 \n",
    "        np.save(embedding_file, E)\n",
    "        \n",
    "    return E\n",
    "\n",
    "EMBED_NUMPY_FILE=\"E.npy\"\n",
    "\n",
    "E = build_embedding_matrix(text_sequences, word2idx, EMBED_NUMPY_FILE)\n",
    "print(f'Embedding matrix: ', E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"spam_classifier_model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      multiple                  2703000   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            multiple                  230656    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  514       \n",
      "=================================================================\n",
      "Total params: 2,934,170\n",
      "Trainable params: 231,170\n",
      "Non-trainable params: 2,703,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, run_mode, embedding_weights, vocab_sz, input_length, embed_sz, \n",
    "                 num_filters=256, kernel_size=3, num_classes=2, **kwargs):\n",
    "        \n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        \n",
    "        if(run_mode == 'scratch'):\n",
    "            self.embedding = tf.keras.layers.Embedding(\n",
    "                input_dim = vocab_sz,\n",
    "                output_dim = embed_sz,\n",
    "                input_length = input_length,\n",
    "                trainable = True)  # pass to layer parent\n",
    "        \n",
    "        elif(run_mode == 'vectorizer'):\n",
    "            self.embedding = tf.keras.layers.Embedding(\n",
    "                input_dim = vocab_sz,\n",
    "                output_dim = embed_sz,\n",
    "                input_length = input_length,\n",
    "                weights = [embedding_weights],  # pass to layer parent\n",
    "                trainable = False) # pass to layer parent\n",
    "\n",
    "        else:  # fine-tune\n",
    "            self.embedding = tf.keras.layers.Embedding(\n",
    "                input_dim = vocab_sz,\n",
    "                output_dim = embed_sz,\n",
    "                input_length = input_length,\n",
    "                weights = [embedding_weights],  # pass to layer parent\n",
    "                trainable = True) # pass to layer parent\n",
    "            \n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, activation='relu')\n",
    "        \n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        \n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        # embedding  => conv => dropout => pool => dense\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "RUN_MODE = \"vectorizer\"\n",
    "EMBED_SIZE = 300\n",
    "model_vectorizer = SpamClassifierModel(run_mode=RUN_MODE, embedding_weights=E, vocab_sz=vocab_size, input_length=max_seqlen, embed_sz=EMBED_SIZE)\n",
    "model_vectorizer.build(input_shape=(None, max_seqlen))\n",
    "model_vectorizer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 55ms/step - loss: 0.4636 - accuracy: 0.8823 - val_loss: 0.1811 - val_accuracy: 0.9531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f18c8558278>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "model_vectorizer.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train model\n",
    "NUM_EPOCHS = 1\n",
    "CLASS_WEIGHTS = {0:1, 1:8 }\n",
    "\n",
    "model_vectorizer.fit(train_dataset, epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    class_weight=CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 1.000\n",
      "confusion matrix\n",
      "[[1118    0]\n",
      " [   0  162]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate against test set\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model_vectorizer.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 모드들의 결과 : from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 81ms/step - loss: 0.9903 - accuracy: 0.6576 - val_loss: 0.3181 - val_accuracy: 0.9479\n",
      "test accuracy: 1.000\n",
      "confusion matrix\n",
      "[[1102    0]\n",
      " [   0  178]]\n"
     ]
    }
   ],
   "source": [
    "RUN_MODE = \"scratch\"\n",
    "\n",
    "model_scratch = SpamClassifierModel(run_mode=RUN_MODE, embedding_weights=None,vocab_sz=vocab_size, input_length=max_seqlen, embed_sz=EMBED_SIZE)\n",
    "model_scratch.build(input_shape=(None, max_seqlen))\n",
    "\n",
    "# compile and train\n",
    "model_scratch.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train model\n",
    "NUM_EPOCHS = 1\n",
    "CLASS_WEIGHTS = {0:1, 1:8 }\n",
    "\n",
    "model_scratch.fit(train_dataset, epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "# evaluate against test set\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model_scratch.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다른 모드들의 결과 : Fine-tue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 81ms/step - loss: 0.4952 - accuracy: 0.8745 - val_loss: 0.0991 - val_accuracy: 0.9818\n",
      "test accuracy: 1.000\n",
      "confusion matrix\n",
      "[[1107    0]\n",
      " [   0  173]]\n"
     ]
    }
   ],
   "source": [
    "RUN_MODE = \"finetue\"\n",
    "\n",
    "model_scratch = SpamClassifierModel(run_mode=RUN_MODE, embedding_weights=E,vocab_sz=vocab_size, input_length=max_seqlen, embed_sz=EMBED_SIZE)\n",
    "model_scratch.build(input_shape=(None, max_seqlen))\n",
    "\n",
    "# compile and train\n",
    "model_scratch.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# train model\n",
    "NUM_EPOCHS = 1\n",
    "CLASS_WEIGHTS = {0:1, 1:8 }\n",
    "\n",
    "model_scratch.fit(train_dataset, epochs=NUM_EPOCHS, \n",
    "    validation_data=val_dataset,\n",
    "    class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "# evaluate against test set\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model_scratch.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
