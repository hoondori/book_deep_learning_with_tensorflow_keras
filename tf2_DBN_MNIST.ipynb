{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2, RBM을 쌓아올려 DBN 형성 후 MNIST 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check GPU available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU[0] is ready')\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)\n",
    "else:\n",
    "    print('Please check GPU available')\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 확보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 784))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 784))\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#Class that defines the behavior of the RBM\n",
    "class RBM(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, lr=1.0, batchsize=100):\n",
    "        \"\"\"\n",
    "        m: Number of neurons in visible layer\n",
    "        n: number of neurons in hidden layer\n",
    "        \"\"\"\n",
    "        #Defining the hyperparameters\n",
    "        self._input_size = input_size #Size of Visible\n",
    "        self._output_size = output_size #Size of outp\n",
    "        self.learning_rate = lr #The step used in gradient descent\n",
    "        self.batchsize = batchsize #The size of how much data will be used for training per sub iteration\n",
    "        \n",
    "        #Initializing weights and biases as matrices full of zeroes\n",
    "        self.w = tf.zeros([input_size, output_size], tf.float32) #Creates and initializes the weights with 0\n",
    "        self.hb = tf.zeros([output_size], tf.float32) #Creates and initializes the hidden biases with 0\n",
    "        self.vb = tf.zeros([input_size], tf.float32) #Creates and initializes the visible biases with 0\n",
    "\n",
    "\n",
    "    #Fits the result from the weighted visible layer plus the bias into a sigmoid curve\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        #Sigmoid \n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "\n",
    "    #Fits the result from the weighted hidden layer plus the bias into a sigmoid curve\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    #Generate the sample probability\n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random.uniform(tf.shape(probs))))\n",
    "\n",
    "    #Training method for the model\n",
    "    def train(self, X, epochs=5):\n",
    "               \n",
    "        loss = []\n",
    "        for epoch in range(epochs):\n",
    "            #For each step/batch\n",
    "            for start, end in zip(range(0, len(X), self.batchsize),range(self.batchsize,len(X), self.batchsize)):\n",
    "                batch = X[start:end]\n",
    "                    \n",
    "                #Initialize with sample probabilities\n",
    "                    \n",
    "                h0 = self.sample_prob(self.prob_h_given_v(batch, self.w, self.hb))\n",
    "                v1 = self.sample_prob(self.prob_v_given_h(h0, self.w, self.vb))\n",
    "                h1 = self.prob_h_given_v(v1, self.w, self.hb)\n",
    "                    \n",
    "                #Create the Gradients\n",
    "                positive_grad = tf.matmul(tf.transpose(batch), h0)\n",
    "                negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "                    \n",
    "                #Update learning rates \n",
    "                self.w = self.w + self.learning_rate *(positive_grad - negative_grad) / tf.dtypes.cast(tf.shape(batch)[0],tf.float32)\n",
    "                self.vb = self.vb +  self.learning_rate * tf.reduce_mean(batch - v1, 0)\n",
    "                self.hb = self.hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "                    \n",
    "            #Find the error rate\n",
    "            err = tf.reduce_mean(tf.square(batch - v1))\n",
    "            print ('Epoch: %d' % epoch,'reconstruction error: %f' % err)\n",
    "            loss.append(err)\n",
    "                    \n",
    "        return loss\n",
    "        \n",
    "    #Create expected output for our DBN\n",
    "    def rbm_output(self, X):\n",
    "        out = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\n",
    "        return out\n",
    "    \n",
    "    def rbm_reconstruct(self,X):\n",
    "        h = tf.nn.sigmoid(tf.matmul(X, self.w) + self.hb)\n",
    "        reconstruct = tf.nn.sigmoid(tf.matmul(h, tf.transpose(self.w)) + self.vb)\n",
    "        return reconstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBN 모델 정의 및 layer-wise 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM:  0   784 -> 500\n",
      "RBM:  1   500 -> 200\n",
      "RBM:  2   200 -> 50\n",
      "New RBM:\n",
      "Epoch: 0 reconstruction error: 0.055611\n",
      "Epoch: 1 reconstruction error: 0.050095\n",
      "Epoch: 2 reconstruction error: 0.048614\n",
      "Epoch: 3 reconstruction error: 0.046413\n",
      "Epoch: 4 reconstruction error: 0.046385\n",
      "New RBM:\n",
      "Epoch: 0 reconstruction error: 0.025836\n",
      "Epoch: 1 reconstruction error: 0.022822\n",
      "Epoch: 2 reconstruction error: 0.020594\n",
      "Epoch: 3 reconstruction error: 0.020283\n",
      "Epoch: 4 reconstruction error: 0.021231\n",
      "New RBM:\n",
      "Epoch: 0 reconstruction error: 0.053708\n",
      "Epoch: 1 reconstruction error: 0.054093\n",
      "Epoch: 2 reconstruction error: 0.052034\n",
      "Epoch: 3 reconstruction error: 0.050597\n",
      "Epoch: 4 reconstruction error: 0.051988\n"
     ]
    }
   ],
   "source": [
    "RBM_hidden_sizes = [500, 200 , 50 ] #create 2 layers of RBM \n",
    "\n",
    "#Since we are training, set input as training data\n",
    "inpX = x_train\n",
    "\n",
    "#Create list to hold our RBMs\n",
    "rbm_list = []\n",
    "\n",
    "#Size of inputs is the number of inputs in the training set\n",
    "input_size = x_train.shape[1]\n",
    "\n",
    "#For each RBM we want to generate\n",
    "for i, size in enumerate(RBM_hidden_sizes):\n",
    "    print ('RBM: ',i,' ',input_size,'->', size)\n",
    "    rbm_list.append(RBM(input_size, size))\n",
    "    input_size = size\n",
    "    \n",
    "#For each RBM in our list\n",
    "for rbm in rbm_list:\n",
    "    print ('New RBM:')\n",
    "    #Train a new one\n",
    "    rbm.train(tf.cast(inpX,tf.float32)) \n",
    "    #Return the output layer\n",
    "    inpX = rbm.rbm_output(inpX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
