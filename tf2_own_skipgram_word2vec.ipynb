{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF2 으로 skipgram 기반 word2vec 모델을 구현하고 학습하기\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/text/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "from tensorflow import feature_column as fc\n",
    "import tensorflow_datasets as tfds\n",
    "plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
    "TENSORBOARD_BINARY = '/home/hoondori/anaconda3/envs/ai/bin/tensorboard'\n",
    "os.environ['TENSORBOARD_BINARY'] =  TENSORBOARD_BINARY\n",
    "%load_ext tensorboard\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [연습] 스킵그램 및 네거티브 샘플링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "# 어휘 목록\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'the',\n",
       " 2: 'wide',\n",
       " 3: 'road',\n",
       " 4: 'shimmered',\n",
       " 5: 'in',\n",
       " 6: 'hot',\n",
       " 7: 'sun'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 역인덱스\n",
    "word2idx = {v:k for k,v in vocab.items()}\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 1, 6, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 to integers\n",
    "example_sentence = [vocab[token] for token in tokens]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스킵그램 데이터 생성 - positive examples\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    example_sentence, vocab_size, window_size=2,\n",
    "    negative_samples=0\n",
    ")\n",
    "len(positive_skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2] -> (shimmered, wide)\n",
      "[6, 7] -> (hot, sun)\n",
      "[2, 3] -> (wide, road)\n",
      "[3, 1] -> (road, the)\n",
      "[2, 4] -> (wide, shimmered)\n"
     ]
    }
   ],
   "source": [
    "for skip_gram in positive_skip_grams[:5]:\n",
    "    print(f'{skip_gram} -> ({word2idx[skip_gram[0]]}, {word2idx[skip_gram[1]]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 1 4 0], shape=(4,), dtype=int64)\n",
      "['in', 'the', 'shimmered', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# 스킵그램 데이터 생성 - negative examples\n",
    "\n",
    "SEED=1\n",
    "\n",
    "# Set the number of negative samples per positive context. \n",
    "num_ns = 4\n",
    "\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class, # class that should be sampled as 'positive'\n",
    "    num_true=1, # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns, # number of negative context words to sample\n",
    "    unique=True, # all the negative samples should be unique\n",
    "    range_max=vocab_size, # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED, # seed for reproducibility\n",
    "    name=\"negative_sampling\" # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([word2idx[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 target word 에 대해서 positive(1) pair와 negative pair(1) 를 배치 \n",
    "\n",
    "# (num_ns, ) -> (num_ns, 1)\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "# (num_ns+1, 1) : single positive followed by negatives\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "label = tf.constant([1] + [0]*num_ns, dtype='int64')\n",
    "\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 4\n",
      "target_word     : shimmered\n",
      "context_indices : [2 5 1 4 0]\n",
      "context_words   : ['wide', 'in', 'the', 'shimmered', '<pad>']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {word2idx[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[word2idx[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : tf.Tensor(4, shape=(), dtype=int32)\n",
      "context : tf.Tensor([2 5 1 4 0], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"target  :\", target)\n",
    "print(f\"context :\", context )\n",
    "print(f\"label   :\", label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "# 스킵그램 샘플링 테이블 \n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 말뭉치 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First Citizen:', shape=(), dtype=string)\n",
      "tf.Tensor(b'Before we proceed any further, hear me speak.', shape=(), dtype=string)\n",
      "tf.Tensor(b'All:', shape=(), dtype=string)\n",
      "tf.Tensor(b'Speak, speak.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for text in text_ds.take(4):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8090 [b'the', b'and', b'to', b'i', b'of', b'you', b'my', b'a', b'that', b'in', b'is', b'not', b'for', b'with', b'me', b'it', b'be', b'your', b'his', b'this']\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization을 이용한 문장 벡터화\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')    \n",
    "\n",
    "vocab_size = 8092\n",
    "sequence_length = 10\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = custom_standardization,\n",
    "    max_tokens = vocab_size-1,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "vectorize_layer.adapt(text_ds.batch(1024))  # fit에 해당 \n",
    "\n",
    "# save the created voca for reference\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(len(inverse_vocab), inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize layer를 적용하려면 string, () shape을 shape=(1, )로 만들어야 한다.\n",
    "# vectorize layer 적용 후에 다시 squeeze해서 원래대로 돌린다.\n",
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    #return vectorize_layer(text)\n",
    "    return tf.squeeze(vectorize_layer(text))\n",
    "\n",
    "text_vector_ds = text_ds.batch(1024).map(vectorize_text).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in text_vector_ds:\n",
    "    values = list(vector.numpy())\n",
    "    if vocab_size in values:\n",
    "        print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] -> [b'queen', b'bring', b'the', b'the', b'the', b'the', b'the', b'the', b'the', b'the']\n",
      "[138  36 982 144 673 125  16 106   0   0] -> [b'both', b'her', b'morrow', b'away', b'court', b'god', b'be', b'york', b'the', b'the']\n",
      "[34  0  0  0  0  0  0  0  0  0] -> [b'we', b'the', b'the', b'the', b'the', b'the', b'the', b'the', b'the', b'the']\n",
      "[106 106   0   0   0   0   0   0   0   0] -> [b'york', b'york', b'the', b'the', b'the', b'the', b'the', b'the', b'the', b'the']\n"
     ]
    }
   ],
   "source": [
    "# 몇가지 옉시\n",
    "\n",
    "for seq in text_vector_ds.take(4):\n",
    "    print(f'{seq} -> {[ inverse_vocab[i] for i in seq ]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777 10\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences), len(sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32777/32777 [00:05<00:00, 5929.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81583 81583 81583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 예제 생성\n",
    "\n",
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        \n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence, \n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples \n",
    "        # with positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1, \n",
    "                num_sampled=num_ns, \n",
    "                unique=True, \n",
    "                range_max=vocab_size, \n",
    "                seed=SEED, \n",
    "                name=\"negative_sampling\")\n",
    "            \n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "            negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels\n",
    "    \n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences, \n",
    "    window_size=2, \n",
    "    num_ns=4, \n",
    "    vocab_size=vocab_size, \n",
    "    seed=SEED)\n",
    "print(len(targets), len(contexts), len(labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 dataset으로 재구성\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 1.6083 - accuracy: 0.2284\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 1.5839 - accuracy: 0.6060\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 1.5158 - accuracy: 0.6331\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 1.4030 - accuracy: 0.6126\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 1.2783 - accuracy: 0.6353\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 1.1582 - accuracy: 0.6755\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 1.0461 - accuracy: 0.7195\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.9441 - accuracy: 0.7578\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.8519 - accuracy: 0.7918\n",
      "Epoch 10/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.7691 - accuracy: 0.8181\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.6957 - accuracy: 0.8405\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.6297 - accuracy: 0.8594\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.5714 - accuracy: 0.8744\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.5201 - accuracy: 0.8878\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.4749 - accuracy: 0.8996\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.4347 - accuracy: 0.9100\n",
      "Epoch 17/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3997 - accuracy: 0.9189\n",
      "Epoch 18/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3682 - accuracy: 0.9270\n",
      "Epoch 19/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3404 - accuracy: 0.9342\n",
      "Epoch 20/20\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3160 - accuracy: 0.9393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff8b8c01780>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        self.target_embedding = Embedding(vocab_size, embedding_dim,\n",
    "                                        input_length=1)\n",
    "        self.context_embedding = Embedding(vocab_size, embedding_dim,\n",
    "                                        input_length=num_ns+1)       \n",
    "        self.dots = Dot(axes=(3,2))\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        \n",
    "        target_embed = self.target_embedding(target)\n",
    "        context_embed = self.context_embedding(context)\n",
    "        dots = self.dots([context_embed, target_embed])\n",
    "        return self.flatten(dots)\n",
    "\n",
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"/tmp/logs/word2vec\")\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
